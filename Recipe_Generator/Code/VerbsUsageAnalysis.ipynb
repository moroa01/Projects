{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VkEUL9ypjmY2"
   },
   "outputs": [],
   "source": [
    "!pip install transformers\n",
    "!pip install torch\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2DuxkRtFjps9"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "zcsRCDYByx37"
   },
   "outputs": [],
   "source": [
    "mydir = \"/content/drive/MyDrive/Dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the data structure containing the information about verbs relation using the library pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "2EhfG4DGj0vZ"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "z = open(mydir + \"relation_dict_completo.pkl\",'rb')\n",
    "relation_dict = pickle.load(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BvRsY9Odj4IT"
   },
   "source": [
    "## Remember that the most used verb with the ingredients is \"Add\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdvQw1A5j_q6"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "model_name= \"moro01525/T5_FineTuning\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "EM3GdZ1aaY8y"
   },
   "outputs": [],
   "source": [
    "final = pd.read_csv(mydir + \"intermedio.csv\").drop(columns=[\"Unnamed: 0\"])\n",
    "final['ingredients'] = 'Ingredients: ' + final['ingredients']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xgkjo30yarc5"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "final[:100000] = final[:100000].sample(frac=1).reset_index(drop=True)\n",
    "dataset = Dataset.from_pandas(final[:1000])\n",
    "evaluation = Dataset.from_pandas(final[101000:106000].sample(frac=1).reset_index(drop=True)[:20])\n",
    "test = final[100000:101000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "T3xucalyawh7"
   },
   "outputs": [],
   "source": [
    "model_dir = mydir + \"T5_FineTuning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LRPWiJ_oa0PO"
   },
   "outputs": [],
   "source": [
    "def generate_recipe(model, ingredients):\n",
    "    input_text = f\"Ingredients: {ingredients}\"\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
    "\n",
    "    outputs = model.generate(input_ids, max_length=150, num_beams=5, repetition_penalty=2.5, no_repeat_ngram_size=2, early_stopping=True)\n",
    "    recipe = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    return recipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### To avoid wasting time generating the sentence every time I save 300 rows of generated text, with relative input and expected text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lxAJCDPaadvk"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "#This will be used for future evaluation\n",
    "x = pd.DataFrame(columns=[\"input\", \"text\", \"label\"])\n",
    "seen = []\n",
    "for i in range(0, 300):\n",
    "  flag = True\n",
    "  while(flag):\n",
    "    index = random.randint(0, 999) + 100000\n",
    "    if(index not in seen):\n",
    "      seen.append(index)\n",
    "      flag = False\n",
    "  input = test.loc[index][\"ingredients\"]\n",
    "  generated_text = generate_recipe(input)\n",
    "  reference_text = test.loc[index][\"steps\"]\n",
    "\n",
    "  x.loc[index, \"input\"] = input\n",
    "  x.loc[index, \"text\"] = generated_text\n",
    "  x.loc[index, \"label\"] = reference_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SgjwIKhDymME"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(mydir + \"generations.pkl\", \"wb\") as f:\n",
    "    pickle.dump(x, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfCuKv9UzVQg"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mi29ZgXxRxkj"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "x = open(mydir + \"generations.pkl\",'rb')\n",
    "x = pickle.load(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "80p8BUv3uTtB"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "z = open(mydir + \"relation_dict_completo.pkl\",'rb')\n",
    "relation_dict = pickle.load(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaV7O-i0zs5-"
   },
   "source": [
    "## To evaluate the model I created a metric:\n",
    "For every generated sentences:\n",
    "\n",
    "*   If the verb is 'add' (the most used) the model gain 0.75 point\n",
    "*   If the verb is contained in the top 10 most used verbs of the model gain 1 full point\n",
    "*   0 in the other cases\n",
    "\n",
    "Finally the score is averaged\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1etfYnPxC0bh"
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def find_conjuncts(token):\n",
    "    conjuncts = [token]\n",
    "    for child in token.children:\n",
    "        if child.dep_ == \"conj\":\n",
    "            conjuncts.extend(find_conjuncts(child))\n",
    "    return conjuncts\n",
    "\n",
    "def get_relations(text):\n",
    "  relations = {}\n",
    "\n",
    "  for p in text.split(\"; \"):\n",
    "    doc = nlp(p)\n",
    "    for token in doc:\n",
    "      if token.pos_ == \"VERB\":\n",
    "        objects = []\n",
    "        for child in token.children:\n",
    "            if child.dep_ in (\"dobj\", \"obj\", \"obl\"):\n",
    "                objects.extend(find_conjuncts(child))\n",
    "        relations[token.lemma_] = [obj.lemma_ for obj in objects]\n",
    "  return relations\n",
    "\n",
    "def get_score(relationships, relation_dict):\n",
    "  score = 0\n",
    "  n = 0\n",
    "  for verb in relationships.keys():\n",
    "    ingredients = relationships[verb]\n",
    "    if(len(ingredients)>0):\n",
    "      for ingredient in ingredients:\n",
    "        n += 1\n",
    "        if(verb == \"add\"):            #The most used verb gives a light penalty\n",
    "          score += 0.75\n",
    "          continue\n",
    "        #If the verb is contained in the top 10 most used verbs of the object then the model gain a full point\n",
    "        verbs = sorted(relation_dict[ingredient].items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "        contained = False\n",
    "        for i in verbs:\n",
    "          if(verb in i):\n",
    "            contained = True\n",
    "        if(contained):\n",
    "          score += 1\n",
    "\n",
    "  if(n != 0):\n",
    "    return score/n\n",
    "  else:\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_4QDn1mYncpR"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "relationships = []\n",
    "scores = []\n",
    "\n",
    "# Iterate through the sentences generated\n",
    "\n",
    "for i in range(0, 300):\n",
    "  input = x.loc[i][\"input\"]\n",
    "  output = x.loc[i][\"label\"]\n",
    "  predict = x.loc[i][\"text\"]\n",
    "\n",
    "  relationships = get_relations(predict)\n",
    "  score = get_score(relationships, relation_dict)\n",
    "  scores.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dELUoj7YTZ8l",
    "outputId": "acb30904-b58d-482c-d1c7-34c78b3f3059"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len: 300\n",
      "final score:  0.8587559523809525\n"
     ]
    }
   ],
   "source": [
    "print(\"Len:\", len(scores))\n",
    "res_sum = sum(scores)\n",
    "print(\"final score: \", res_sum/len(scores))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJD8ij_W039r"
   },
   "source": [
    "### The final score:\n",
    "\n",
    "*   0.8587559523809525\n",
    "\n",
    "\n",
    "\n",
    "### If 'add' gives 1 full point:\n",
    "*   0.9622010582010586"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
