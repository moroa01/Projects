{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VkEUL9ypjmY2"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "2DuxkRtFjps9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mydir = \"/content/drive/MyDrive/Dataset/\""
      ],
      "metadata": {
        "id": "zcsRCDYByx37"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "z = open(mydir + \"relation_dict_completo.pkl\",'rb')\n",
        "relation_dict = pickle.load(z)"
      ],
      "metadata": {
        "id": "2EhfG4DGj0vZ"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Remember that the most used verb with the ingredients is \"Add\"\n"
      ],
      "metadata": {
        "id": "BvRsY9Odj4IT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "model_name= \"moro01525/T5_FineTuning\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ],
      "metadata": {
        "id": "rdvQw1A5j_q6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "final = pd.read_csv(mydir + \"intermedio.csv\").drop(columns=[\"Unnamed: 0\"])\n",
        "final['ingredients'] = 'Ingredients: ' + final['ingredients']"
      ],
      "metadata": {
        "id": "EM3GdZ1aaY8y"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import Dataset\n",
        "final[:100000] = final[:100000].sample(frac=1).reset_index(drop=True)\n",
        "dataset = Dataset.from_pandas(final[:1000])\n",
        "evaluation = Dataset.from_pandas(final[101000:106000].sample(frac=1).reset_index(drop=True)[:20])\n",
        "test = final[100000:101000]"
      ],
      "metadata": {
        "id": "Xgkjo30yarc5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = mydir + \"T5_FineTuning\""
      ],
      "metadata": {
        "id": "T3xucalyawh7"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_dir)"
      ],
      "metadata": {
        "id": "7jirPuyNaxDo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_recipe(model, ingredients):\n",
        "    input_text = f\"Ingredients: {ingredients}\"\n",
        "    input_ids = tokenizer.encode(input_text, return_tensors=\"pt\")\n",
        "\n",
        "    outputs = model.generate(input_ids, max_length=150, num_beams=5, repetition_penalty=2.5, no_repeat_ngram_size=2, early_stopping=True)\n",
        "    recipe = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return recipe"
      ],
      "metadata": {
        "id": "LRPWiJ_oa0PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import random\n",
        "\n",
        "#To avoid wasting time generating the  every time I save 300 rows of generated text, with relative input and expected text\n",
        "#This will be used for future evaluation\n",
        "x = pd.DataFrame(columns=[\"input\", \"text\", \"label\"])\n",
        "seen = []\n",
        "for i in range(0, 300):\n",
        "  flag = True\n",
        "  while(flag):\n",
        "    index = random.randint(0, 999) + 100000\n",
        "    if(index not in seen):\n",
        "      seen.append(index)\n",
        "      flag = False\n",
        "  input = test.loc[index][\"ingredients\"]\n",
        "  generated_text = generate_recipe(input)\n",
        "  reference_text = test.loc[index][\"steps\"]\n",
        "\n",
        "  x.loc[index, \"input\"] = input\n",
        "  x.loc[index, \"text\"] = generated_text\n",
        "  x.loc[index, \"label\"] = reference_text"
      ],
      "metadata": {
        "id": "lxAJCDPaadvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "with open(mydir + \"generations.pkl\", \"wb\") as f:\n",
        "    pickle.dump(x, f)"
      ],
      "metadata": {
        "id": "SgjwIKhDymME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Analysis"
      ],
      "metadata": {
        "id": "xfCuKv9UzVQg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "x = open(mydir + \"generations.pkl\",'rb')\n",
        "x = pickle.load(x)"
      ],
      "metadata": {
        "id": "Mi29ZgXxRxkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "z = open(mydir + \"relation_dict_completo.pkl\",'rb')\n",
        "relation_dict = pickle.load(z)"
      ],
      "metadata": {
        "id": "80p8BUv3uTtB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###To evaluate the model I created a metric:\n",
        "For every generated sentences:\n",
        "\n",
        "*   If the verb is 'add' the model gain 0.75 point\n",
        "*   If the verb is contained in the top 10 most used verbs of the model gain 1 full point\n",
        "*   0 in the other cases\n",
        "\n",
        "Finally the score is averaged\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "uaV7O-i0zs5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def find_conjuncts(token):\n",
        "    conjuncts = [token]\n",
        "    for child in token.children:\n",
        "        if child.dep_ == \"conj\":\n",
        "            conjuncts.extend(find_conjuncts(child))\n",
        "    return conjuncts\n",
        "\n",
        "def get_relations(text):\n",
        "  relations = {}\n",
        "\n",
        "  for p in text.split(\"; \"):\n",
        "    doc = nlp(p)\n",
        "    for token in doc:\n",
        "      if token.pos_ == \"VERB\":\n",
        "        objects = []\n",
        "        for child in token.children:\n",
        "            if child.dep_ in (\"dobj\", \"obj\", \"obl\"):\n",
        "                objects.extend(find_conjuncts(child))\n",
        "        relations[token.lemma_] = [obj.lemma_ for obj in objects]\n",
        "  return relations\n",
        "\n",
        "def get_score(relationships, relation_dict):\n",
        "  score = 0\n",
        "  n = 0\n",
        "  for verb in relationships.keys():\n",
        "    ingredients = relationships[verb]\n",
        "    if(len(ingredients)>0):\n",
        "      for ingredient in ingredients:\n",
        "        n += 1\n",
        "        if(verb == \"add\"):            #The most used verb gives a light penalty\n",
        "          score += 0.75\n",
        "          continue\n",
        "        #If the verb is contained in the top 10 most used verbs of the object then the model gain a full point\n",
        "        verbs = sorted(relation_dict[ingredient].items(), key=lambda x: x[1], reverse=True)[:10]\n",
        "        contained = False\n",
        "        for i in verbs:\n",
        "          if(verb in i):\n",
        "            contained = True\n",
        "        if(contained):\n",
        "          score += 1\n",
        "\n",
        "  if(n != 0):\n",
        "    return score/n\n",
        "  else:\n",
        "    return 1"
      ],
      "metadata": {
        "id": "1etfYnPxC0bh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "relationships = []\n",
        "scores = []\n",
        "\n",
        "# Iterate through the sentences generated\n",
        "\n",
        "for i in range(0, 300):\n",
        "  input = x.loc[i][\"input\"]\n",
        "  output = x.loc[i][\"label\"]\n",
        "  predict = x.loc[i][\"text\"]\n",
        "\n",
        "  relationships = get_relations(predict)\n",
        "  score = get_score(relationships, relation_dict)\n",
        "  scores.append(score)"
      ],
      "metadata": {
        "id": "_4QDn1mYncpR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Len:\", len(scores))\n",
        "res_sum = sum(scores)\n",
        "print(\"final score: \", res_sum/len(scores))\n",
        "#con prime 10 e add 1:     0.9622010582010586\n",
        "#con prime 10 e add 0.75:  0.8587559523809525\n",
        "#con prime 7 e add 1:      0.9622010582010586\n",
        "#con prime 7 e add 0.75:   0.8444226190476191\n",
        "#con prime 5 e add 0.75:   0.8177162698412701"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dELUoj7YTZ8l",
        "outputId": "acb30904-b58d-482c-d1c7-34c78b3f3059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Len: 300\n",
            "final score:  0.8587559523809525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##The final score:\n",
        "\n",
        "*   0.8587559523809525\n",
        "\n",
        "\n",
        "\n",
        "##If 'add' gives 1 full point:\n",
        "*   0.9622010582010586"
      ],
      "metadata": {
        "id": "BJD8ij_W039r"
      }
    }
  ]
}