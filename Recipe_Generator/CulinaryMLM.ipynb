{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0i1YNnHXdzX"
      },
      "outputs": [],
      "source": [
        "!pip install transformers\n",
        "!pip install torch\n",
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "mydir = \"/content/drive/MyDrive/Dataset/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FIaW46Ric-5",
        "outputId": "af6c0216-42aa-469d-9b63-7ff09af02630"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUDLqOc2ih0O"
      },
      "outputs": [],
      "source": [
        "step = pd.read_csv(mydir + \"RAW_recipes.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9mpgqB5i2v1"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame(columns=[\"text\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SzigX9c4jM0b"
      },
      "outputs": [],
      "source": [
        "#This is a function used select only the clean noisy sentence from the original dataset\n",
        "#the dataset created is used for train the model on culinary context\n",
        "def is_valid_sentence(i):\n",
        "  if(len(i)<=2):\n",
        "    return False\n",
        "  if( (i[0]==\"s\" and i[1]==\" \") or (i[0]==\"t\" and i[1]==\" \") or (i[0]==\"n\" and i[1]==\" \") or i[0]==\",\" or i[0]=='\"' or i[1]=='\"' or i[0]==\":\" or i[0]==\"-\" or i[-1]==\"-\" or i[-1]==\" \" or i[0]==\" \"):\n",
        "    return False\n",
        "  return '?' not in i and '!' not in i and '~' not in i"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FrZKRZHGi4EE"
      },
      "outputs": [],
      "source": [
        "l = len(df)\n",
        "for x in step[\"steps\"][0:200000]:\n",
        "  if(l>150000):                 #this is to get at most ~150k rows\n",
        "    break\n",
        "\n",
        "  for s in x[1:-2].split(\"', \"):\n",
        "    if(is_valid_sentence(s)):\n",
        "      z = len(s.split(\" \"))\n",
        "      if(z>3 and z<20):         #I decided to discard the sentences with too few and too many word\n",
        "        df.loc[len(df)] = s[1:]\n",
        "        l += 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4TtdE4b6kIlC",
        "outputId": "6f76072a-c22c-48cb-d5fa-37f9b35e0f4b"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "150007"
            ]
          },
          "execution_count": 43,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5XqteKB1SGvt"
      },
      "outputs": [],
      "source": [
        "df.to_csv(mydir + \"final.csv\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAdFO4WL_tQ-"
      },
      "source": [
        "##Training with MLM task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HPVyteJ3tVXH"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(mydir + \"final.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WZhyRiTXke9"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "import torch\n",
        "\n",
        "#Load the model and the tokenizer\n",
        "#The first time the loaded model was \"t5-small\"\n",
        "model_name = \"moro01525/T5_MLM\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-small\")\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l15wBHeYYGLq",
        "outputId": "5d884a4c-edbc-47c0-b681-c61266704499"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "60506624"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "model.num_parameters()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0tZzNmeCUV4F"
      },
      "outputs": [],
      "source": [
        "import random, spacy\n",
        "\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "def mask_text(text, mask_token=\"<extra_id_0>\"):\n",
        "    doc = nlp(text)\n",
        "\n",
        "    #select only the NOUN or the VERB for the masking task\n",
        "    candidates = [token.text for token in doc if token.pos_ in ['NOUN', 'VERB']]\n",
        "\n",
        "    #if there is no NOUN or VERB then masks a random word\n",
        "    if not candidates:\n",
        "        candidates = [token.text for token in doc]\n",
        "\n",
        "    #select randomly the words for the masking\n",
        "    words = text.split()\n",
        "    num_masks = random.randint(1, max(1, int(len(words)*0.15)))   #The 15% of the words is masked\n",
        "    masked_words = random.sample(candidates, num_masks)\n",
        "\n",
        "    #mask the word\n",
        "    masked_text = text\n",
        "    for word in masked_words:\n",
        "        masked_text = masked_text.replace(word, mask_token, 1)\n",
        "\n",
        "    return masked_text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DqSGzERJTdQU"
      },
      "outputs": [],
      "source": [
        "class CulinaryMLMDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, texts, tokenizer, max_length=50):\n",
        "        self.texts = texts\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        original_text = self.texts[idx]\n",
        "        masked_text = mask_text(original_text)\n",
        "\n",
        "        inputs = self.tokenizer(masked_text, return_tensors='pt', truncation=True, padding='max_length', max_length=self.max_length)\n",
        "        labels = self.tokenizer(original_text, return_tensors='pt', truncation=True, padding='max_length', max_length=self.max_length).input_ids\n",
        "\n",
        "        input_ids = inputs['input_ids'].squeeze()\n",
        "        attention_mask = inputs['attention_mask'].squeeze()\n",
        "        labels = labels.squeeze()\n",
        "\n",
        "        return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boAWccPfYra4"
      },
      "outputs": [],
      "source": [
        "train = df['text'][0:30000].tolist()          #Every epoch the indexes are changed (30'000 rows every epoch)\n",
        "evaluation = df['text'][95000:100000].tolist()\n",
        "dataset = CulinaryMLMDataset(train, tokenizer, 50)\n",
        "eval = CulinaryMLMDataset(evaluation, tokenizer, 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9fcMPOZYbM5d"
      },
      "outputs": [],
      "source": [
        "model_dir = mydir + \"T5_MLM\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UgC1usduY0kr"
      },
      "outputs": [],
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "#print(model_folder)\n",
        "# Define the training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=model_dir,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-5,\n",
        "    weight_decay=0.01,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=1000,\n",
        "    logging_steps=1000,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset,\n",
        "    eval_dataset=eval\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kXZwbEmYavoJ"
      },
      "outputs": [],
      "source": [
        "trainer.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0ymtII43bvYu"
      },
      "outputs": [],
      "source": [
        "trainer.save_model(model_dir)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4N-4ZZ52kHxV"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub\n",
        "from huggingface_hub import notebook_login\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kTigqW50kL0P"
      },
      "outputs": [],
      "source": [
        "trainer.push_to_hub(commit_message='T5 dopo 4 epoch (finale), rouge1: 0.93, rouge2: 0.88, rougeL: 0.93; BLEU Score: 0.7; Perplexity: 34.5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DiPSdBqnwB7w"
      },
      "outputs": [],
      "source": [
        "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
        "from datasets import load_dataset, load_metric\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EhDn0zsTAXpv"
      },
      "outputs": [],
      "source": [
        "#Create the test dataset with masked word\n",
        "ds = pd.DataFrame(columns=[\"text\"])\n",
        "for i in df[102000:103000][\"text\"]:\n",
        "  text = mask_text(i)\n",
        "  ds.loc[len(ds)] = text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pQasOqvhnvtJ",
        "outputId": "f0a9c1ee-fd45-4230-c0d5-2d75d104995f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Perplexity: 34.53046417236328\n"
          ]
        }
      ],
      "source": [
        "def calculate_perplexity(model, tokenizer, sentences):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for sentence in sentences:\n",
        "            inputs = tokenizer(sentence, return_tensors='pt')\n",
        "            outputs = model(**inputs, labels=inputs['input_ids'])\n",
        "            loss = outputs.loss\n",
        "            total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(sentences)\n",
        "    perplexity = torch.exp(torch.tensor(avg_loss))\n",
        "    return perplexity.item()\n",
        "\n",
        "perplexity = calculate_perplexity(model, tokenizer, ds[\"text\"])\n",
        "print(f'Perplexity: {perplexity}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlPt5JwACIdN",
        "outputId": "c7b67726-2e35-45cc-9713-674881c9080b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BLEU Score: 0.6967825995856721\n"
          ]
        }
      ],
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "def calculate_bleu_score(model, tokenizer, masked_sentences, original_sentences):\n",
        "    model.eval()\n",
        "    total_bleu = 0.0\n",
        "    with torch.no_grad():\n",
        "        for masked, original in zip(masked_sentences, original_sentences):\n",
        "            inputs = tokenizer(masked, return_tensors='pt')\n",
        "            outputs = model.generate(inputs['input_ids'], max_length=50, num_beams=5, early_stopping=True)\n",
        "            decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            reference_tokens = original.split()\n",
        "            output_tokens = decoded_output.split()\n",
        "            bleu_score = sentence_bleu([reference_tokens], output_tokens)\n",
        "            total_bleu += bleu_score\n",
        "\n",
        "    avg_bleu_score = total_bleu / len(masked_sentences)\n",
        "    return avg_bleu_score\n",
        "\n",
        "bleu_score = calculate_bleu_score(model, tokenizer, ds[\"text\"], df[102000:103000][\"text\"])\n",
        "print(f'BLEU Score: {bleu_score}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YFbRWBPOlp3Y"
      },
      "outputs": [],
      "source": [
        "!pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCIfoo01AQg-",
        "outputId": "864f8071-c557-49a3-af0e-bc58f22543d3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ROUGE Scores: {'rouge1': 0.9331053913306484, 'rouge2': 0.8797187884119245, 'rougeL': 0.9329877442718251}\n"
          ]
        }
      ],
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge_score(model, tokenizer, masked_sentences, original_sentences):\n",
        "    model.eval()\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    total_rouge1 = 0.0\n",
        "    total_rouge2 = 0.0\n",
        "    total_rougeL = 0.0\n",
        "    with torch.no_grad():\n",
        "        for masked, original in zip(masked_sentences, original_sentences):\n",
        "            inputs = tokenizer(masked, return_tensors='pt')\n",
        "            outputs = model.generate(inputs['input_ids'], max_length=50, num_beams=5, early_stopping=True)\n",
        "            decoded_output = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            scores = scorer.score(original, decoded_output)\n",
        "            total_rouge1 += scores['rouge1'].fmeasure\n",
        "            total_rouge2 += scores['rouge2'].fmeasure\n",
        "            total_rougeL += scores['rougeL'].fmeasure\n",
        "\n",
        "    avg_rouge1 = total_rouge1 / len(masked_sentences)\n",
        "    avg_rouge2 = total_rouge2 / len(masked_sentences)\n",
        "    avg_rougeL = total_rougeL / len(masked_sentences)\n",
        "    return {'rouge1': avg_rouge1, 'rouge2': avg_rouge2, 'rougeL': avg_rougeL}\n",
        "\n",
        "# Calcola il ROUGE Score\n",
        "rouge_scores = calculate_rouge_score(model, tokenizer, ds[\"text\"], df[102000:103000][\"text\"])\n",
        "print(f'ROUGE Scores: {rouge_scores}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
